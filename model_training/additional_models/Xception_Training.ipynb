{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Xception-Training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOcREWocvZaMK4AbmzMHRV9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agiagoulas/page-stream-segmentation/blob/master/model_training/additional_models/Xception_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iR6ToZveArSi"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZKXHKC87lIn"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyFwONrxBN0J"
      },
      "source": [
        "import csv\n",
        "import math\n",
        "import numpy as np\n",
        "import sklearn.metrics as sklm"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1x96egi5I4Z"
      },
      "source": [
        "from tensorflow.keras.applications import Xception\n",
        "from tensorflow.keras.optimizers import Nadam\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Flatten, Dropout, Dense, LeakyReLU, concatenate\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "from tensorflow.keras.applications.xception import preprocess_input"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkwQPkPZA0Jy"
      },
      "source": [
        "working_dir =  \"/Tobacco800/\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lg2IZGjt-rJ_"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4ZyUmNGAl1P"
      },
      "source": [
        "img_path_template = working_dir + \"Tobacco800_Small/%s.png\"\n",
        "img_dim = (224,224)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYBuDqOeBMMS"
      },
      "source": [
        "def read_csv_data(csvfile):\n",
        "    data_instances = []\n",
        "    prev_image_file = \"\"\n",
        "\n",
        "    # CSV Columns: \"counter\";\"documentText\";\"label\";\"documentName\"\n",
        "    with open(csvfile, 'r', encoding='UTF-8') as f:\n",
        "        datareader = csv.reader(f, delimiter=';', quotechar='\"')\n",
        "        next(datareader)\n",
        "        for counter, csv_row in enumerate(datareader):\n",
        "            data_instances.append([csv_row[0], csv_row[3], prev_image_file, csv_row[2]])\n",
        "            prev_image_file = csv_row[3]\n",
        "        return data_instances"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwETO4uV-pPW"
      },
      "source": [
        "LABEL2IDX = {'FirstPage' : 1, 'NextPage' : 0}\n",
        "class ImageFeatureGenerator(Sequence):\n",
        "    def __init__(self, image_data, img_dim, prevpage=False, batch_size=32):\n",
        "        self.image_data = image_data\n",
        "        self.indices = np.arange(len(self.image_data))\n",
        "        self.batch_size = batch_size\n",
        "        self.img_dim = img_dim\n",
        "        self.prevpage = prevpage\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.image_data) / self.batch_size)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        np.random.shuffle(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inds = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        if self.prevpage:\n",
        "            batch_x = self.process_image_data_prevpage(inds)\n",
        "        else:\n",
        "            batch_x = self.process_image_data(inds)  \n",
        "        batch_y = self.generate_output_labels(inds)\n",
        "        return batch_x, batch_y\n",
        "\n",
        "    def get_image_data_from_file(self, image_id):\n",
        "        image_file = img_path_template % image_id \n",
        "        image = img_to_array(load_img(image_file, target_size=(self.img_dim[0], self.img_dim[1])))\n",
        "        image = preprocess_input(image)\n",
        "        return image\n",
        "\n",
        "    def process_image_data(self, inds):\n",
        "        image_array = []\n",
        "        for index in inds:\n",
        "            image = self.get_image_data_from_file(self.image_data[index][1])\n",
        "            image_array.append(image)\n",
        "        return [np.array(image_array)]\n",
        "\n",
        "    def process_image_data_prevpage(self, inds):\n",
        "        image_array = []\n",
        "        prev_image_array = []\n",
        "\n",
        "        for index in inds:\n",
        "            image = self.get_image_data_from_file(self.image_data[index][1])\n",
        "            image_array.append(image)\n",
        "\n",
        "            if self.image_data[index][2] != \"\":\n",
        "                prev_image = self.get_image_data_from_file(self.image_data[index][2])\n",
        "            else:\n",
        "                prev_image = np.zeros((self.img_dim[0], self.img_dim[1], 3))\n",
        "            prev_image_array.append(prev_image)\n",
        "\n",
        "        return [np.array(image_array), np.array(prev_image_array)]\n",
        "\n",
        "    def generate_output_labels(self, inds):\n",
        "        output_labels = []\n",
        "        for index in inds:\n",
        "            temp_output = LABEL2IDX[self.image_data[index][3]]\n",
        "            output_labels.append(temp_output)\n",
        "        return np.array(output_labels)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGK5lQXs9B73"
      },
      "source": [
        "class ValidationCheckpoint(Callback):\n",
        "    def __init__(self, model, filepath, test_data, img_dim, prev_page_generator=False, metric='kappa'):\n",
        "        self.test_data = test_data\n",
        "        self.img_dim = img_dim\n",
        "        self.metric = metric\n",
        "        self.max_metric = float('-inf')\n",
        "        self.max_metrics = None\n",
        "        self.model = model\n",
        "        self.filepath = filepath\n",
        "        self.history = []\n",
        "        self.prev_page_generator = prev_page_generator\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        \n",
        "        predicted_labels = np.round(self.model.predict(ImageFeatureGenerator(self.test_data, self.img_dim, prevpage=self.prev_page_generator)))\n",
        "        true_labels = [LABEL2IDX[x[3]] for x in self.test_data]\n",
        "\n",
        "        eval_metrics = {\n",
        "            'accuracy' : sklm.accuracy_score(true_labels, predicted_labels),\n",
        "            'f1_micro' : sklm.f1_score(true_labels, predicted_labels, average='micro'),\n",
        "            'f1_macro' : sklm.f1_score(true_labels, predicted_labels, average='macro'),\n",
        "            'f1_binary' : sklm.f1_score(true_labels, predicted_labels, average='binary', pos_label=1),\n",
        "            'kappa' : sklm.cohen_kappa_score(true_labels, predicted_labels)\n",
        "        }\n",
        "        eval_metric = eval_metrics[self.metric]\n",
        "        self.history.append(eval_metric)\n",
        "        \n",
        "        if epoch > -1 and eval_metric > self.max_metric:\n",
        "            print(\"\\n\" + self.metric + \" improvement: \" + str(eval_metric) + \" (before: \" + str(self.max_metric) + \"), saving to \" + self.filepath)\n",
        "            self.max_metric = eval_metric     # optimization target\n",
        "            self.max_metrics = eval_metrics   # all metrics\n",
        "            self.model.save(self.filepath)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfJ9C_795CXO"
      },
      "source": [
        "def compile_model(img_dim):\n",
        "    model_xception = Xception(include_top=False, weights=\"imagenet\", input_shape=(img_dim[0], img_dim[1], 3))\n",
        "\n",
        "    top_model = Flatten()(model_xception.output)\n",
        "    top_model = Dropout(0.5)(top_model)\n",
        "    top_model = Dense(512)(top_model)\n",
        "    top_model = LeakyReLU()(top_model)\n",
        "    top_model = Dropout(0.5)(top_model)\n",
        "    top_model = Dense(256)(top_model)\n",
        "    top_model = LeakyReLU()(top_model)\n",
        "\n",
        "    model_output = Dense(1, activation=\"sigmoid\")(top_model)\n",
        "    model = Model(model_xception.input, model_output)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=Nadam(lr=0.00001), metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBg3ueIR-wYe"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxxKhlJB-v7c"
      },
      "source": [
        "data_train = read_csv_data(working_dir + \"tobacco800.train\")\n",
        "data_test = read_csv_data(working_dir + \"tobacco800.test\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQPpNqzb782W"
      },
      "source": [
        "n_repeats = 10\n",
        "n_epochs = 20\n",
        "metric_history = []\n",
        "optimize_for = 'kappa'\n",
        "\n",
        "with tf.device('/GPU:0'):\n",
        "    for i in range(n_repeats):\n",
        "        print(\"Repeat \" + str(i+1) + \" of \" + str(n_repeats))\n",
        "        print(\"-------------------------\")\n",
        "        model_image = compile_model(img_dim)\n",
        "        model_file = working_dir + \"xception-image_model-%02d.hdf5\" % (i,)\n",
        "        checkpoint = ValidationCheckpoint(model_image, model_file, data_test, img_dim, metric='kappa')\n",
        "        model_image.fit(ImageFeatureGenerator(data_train, img_dim, prevpage=False),\n",
        "                        callbacks=[checkpoint],\n",
        "                        epochs=n_epochs)\n",
        "        metric_history.append(checkpoint.max_metrics)\n",
        "\n",
        "print(metric_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20059NwIBVh5"
      },
      "source": [
        "for i, r in enumerate(metric_history):\n",
        "    model_file = working_dir + \"xception-image_model-%02d.hdf5\" % (i)\n",
        "    print(str(i) + ' ' + str(r['kappa']) + ' ' + str(r['accuracy']) + ' ' + str(r['f1_micro']) + ' ' + str(r['f1_macro']) + ' ' +  model_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV0BXWorr-Ut",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07dc7600-2101-4983-e8d7-386d4cc7d484"
      },
      "source": [
        "_, _, _, y_true = zip(*data_test)\n",
        "y_true = [1 if y == 'FirstPage' else 0 for y in y_true]\n",
        "model_image = load_model(working_dir + \"xception-image_model-09.hdf5\")\n",
        "y_predict = np.round(model_image.predict(ImageFeatureGenerator(data_test, img_dim, prevpage=False)))\n",
        "print(\"Accuracy: \" + str(sklm.accuracy_score(y_true, y_predict)))\n",
        "print(\"Kappa: \" + str(sklm.cohen_kappa_score(y_true, y_predict)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8532818532818532\n",
            "Kappa: 0.6923221207952983\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}